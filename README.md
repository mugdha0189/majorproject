# Major Project

ABSTRACT
Speech and voice recognition technology have advanced to great levels over the past few years, however, this advancement mainly benefits speakers with typical speech patterns and tends to ignore the part of the population with speech disabilities such as Dysarthria and the issues that they face whilst using these technologies and tools. This proposed system aims to make speech recognition technology accessible to such people by creating an LPC model - using deep learning concepts, that can translate disordered speech into text and then turn that text into automated speech so that it can be understood by everyone. We extract acoustic features from the audio samples and run them through the LPC model to determine which gives us the best accuracy. Existing models have performed comparisons on and applied existing speech recognition models that are meant for regular speech on dysarthric voice samples and achieved minimal accuracy (i.e., in the range of 30% to 50%) in their recognition. The dataset is taken from the TORGO dysarthric speech corpus which includes audio samples of phrases spoken by patients of Dysarthria and the LPC model is expected to recognize words spoken by dysarthric patients and translate dysarthric speech into automated speech with accuracy in the range of 60% to 95%.
